\documentclass[xcolor=svgnames, t]{beamer}

\usepackage[utf8]{inputenc}
% \usepackage{booktabs, comment}
\usepackage[absolute, overlay]{textpos}
\useoutertheme{infolines} 
% \usepackage{csquotes}
\usepackage{enumerate}
\usepackage[french]{babel}
% \usepackage[citestyle=authoryear,backend=bibtex,citetracker=true]{biblatex}
% \bibliography{bibfile}

% \usepackage{mathabx}
% \newcommand{\stup}{{\scriptsize $\blacktriangle$ }}
% \newcommand{\stdown}{$\blacktriangledown$ }
\newcommand{\eg}{\textit{e.g. }}
\newcommand{\ie}{\textit{i.e. }}
\renewcommand{\footnotesize}{\tiny}
\newcommand{\coloredemph}[1]{\textcolor{internationalblue}{\emph{#1}}}
% \newcommand{\Rlogo}{\includegraphics[scale = 0.02]{images/Rlogo.png}}
% \newcommand{\Pythonlogo}{\includegraphics[scale = 0.075]{images/Pythonlogo.png}}

% math commands
\usepackage{amsmath}
\newcommand{\norm}[2]{\lVert #1 \rVert_{#2}}
\newcommand{\dictx}{\mathbf{\Theta}_{X_n}}
\newcommand{\vectorx}[1]{\boldsymbol{\MakeLowercase{\mathbf{#1}}}}
\newcommand{\matrixx}[1]{\boldsymbol{\MakeUppercase{#1}}}
\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}}
\newcommand{\argmax}{\mathop{\mathrm{arg\,max}}}
\newcommand{\argminx}[1]{\arg\min_{#1}}
\newcommand{\dotprod}[2]{\langle #1, #2 \rangle}

% Images location
\graphicspath{ {./images/} }

\usepackage{textpos}
\definecolor{myuniversity}{RGB}{36, 42, 117}
\definecolor{internationalblue}{RGB}{48, 157, 181}
\definecolor{dodgerblue}{RGB}{91, 193, 213}
\usecolortheme[named=myuniversity]{structure}
\usepackage{tikz}

\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}

\usetheme{Madrid}

\logo{\includegraphics[scale=0.25]{Thales.png}}
\setbeamercolor{title in head/foot}{bg=internationalblue}
\setbeamercolor{author in head/foot}{bg=dodgerblue}

\title[Introduction aux Processus Gaussiens]{Introduction aux Processus Gaussiens}
\subtitle{Application aux données spatio-temporelles}
\institute[]{}
% \titlegraphic{
% 	\includegraphics[scale=0.5]{Thales.png}
% % 	\includegraphics[height=1.5cm]{images/UT3_PRES_logoQ.png}
% }
\author[Cl\'ement Lejeune]{Cl\'ement Lejeune}

\institute[TSN/AD/AD3/IA]{
Thales Services Numériques,
\\ AD/AD3/IA
}
\date{\today}
% \date{25 Novembre 2024}

\addtobeamertemplate{navigation symbols}{}{%
	\usebeamerfont{footline}%
	\usebeamercolor[fg]{footline}%
	\hspace{1em}%
	\insertframenumber/\inserttotalframenumber
}

% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{corollary}{Corollary}[theorem]
% \newtheorem{lemma}[theorem]{Lemma}


\begin{document}

%========== First frame ==================%
%Information to be included in the title page:
\frame{\titlepage}

%========== ToC ==========================%
% \AtBeginSection[]
{
  \begin{frame}
    \frametitle{Plan}
    \tableofcontents[currentsection]
  \end{frame}
}

%========== Gaussianity ==================%
\section{Gaussien: vecteur vs. processus}
\begin{frame}
  \frametitle{\secname}
% 
  Loi Gaussienne unidiemensionnelle:
  \begin{equation*}
    y \sim \mathcal{N}(m, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(y-m)^2}{2 \sigma^2}}
  \end{equation*}

  \begin{enumerate}
    \item $m$: espérance (aka moyenne) de $y$
    \item $\sigma > 0$: écart-type
  \end{enumerate}

  % \pause

  \begin{figure}
    \includegraphics[scale=0.4]{gaussian_1d.pdf}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{\secname}

  Loi Gaussienne multidimensionnelle (\coloredemph{vecteur} Gaussien): Distribution \coloredemph{jointe} des composantes d'un vecteur $d$-dimensionnel dont les \coloredemph{marginales} sont Gaussiennes (unidimensionnelles).
  \begin{equation*}
    \vectorx{Y} := [ y_1, \dots, y_d ]^\top  \sim \mathcal{N}(\vectorx{\mu} , \matrixx{\Sigma}) =  \frac{1}{\sqrt{(2 \pi)^d \det |\matrixx{\Sigma}|}} e^{-\frac{1}{2}(\vectorx{y - \mu})^\top \matrixx{\Sigma}^{-1} (\vectorx{y - \mu})}
  \end{equation*}
% 
  \begin{enumerate}
    \item $\vectorx{\mu} \in \mathbb{R}^d$: \coloredemph{vecteur moyen} $\implies \mu_j$: moyenne de la Gaussienne $y_j$
    \item $\matrixx{\Sigma} \in \mathbb{R}^{d \times d}$ définie positive\footnote{i.e. $\vectorx{a}^\top \matrixx{\Sigma} \vectorx{a} > 0$ (donc symmétrique)}: \coloredemph{matrice de covariance}
  \end{enumerate}
% 
  \pause
  \begin{equation*}
    \matrixx{\Sigma}
    =
    \begin{pmatrix} 
      \Sigma^2_{1}  & \Sigma_{{1}{2}} &  \cdots & \Sigma_{{1}{d}} \\
      \Sigma_{{2}{1}} & \ddots          & \cdots  & \vdots \\
      \vdots          & \vdots          & \ddots  & \vdots \\
      \Sigma_{{d}{1}} & \cdots          & \cdots  &  \Sigma^2_{d} 
      \end{pmatrix}
    \implies
    \left\{
      \begin{array}{ll}
        \Sigma_j  : \text{écart-type} \text{ de } y_j\\
        \Sigma_{ij} : \text{covariance entre les } \\
        \text{marginales de } y_i \text{ et } y_j
      \end{array}
    \right.
  \end{equation*}

  % $\mathcal{N}(\mu_1, \Sigma_1^2) \dots \mathcal{N}(\mu_d, \Sigma_d^2)$.
\end{frame}

% d=2, low correlation
\begin{frame}
  % \frametitle{\secname}
  Cas $d=2$:
  \begin{equation*}
    \vectorx{\mu}
    =
    \begin{pmatrix}
      \mu_1 \\
      \mu_2
    \end{pmatrix},
    \quad
    \matrixx{\Sigma}
    =
      \begin{pmatrix}
        \Sigma_{1} & \Sigma_{12} \\
        \Sigma_{21} & \Sigma_{2}
      \end{pmatrix}
    =
      \begin{pmatrix}
        1.5 & \color{red}\rho = -0.3 \\
        \color{red}\rho = -0.3 & 1.5
      \end{pmatrix}
  \end{equation*}
% 
  \begin{figure}
    \includegraphics[scale=0.4]{gaussian_2d_rho_low.pdf}
  \end{figure}
\end{frame}

% d=2, high correlation
\begin{frame}
  Cas $d=2$:
  %
  \begin{equation*}
    \vectorx{\mu}
    =
    \begin{pmatrix}
      \mu_1 \\
      \mu_2
    \end{pmatrix},
    \quad
    \matrixx{\Sigma}
    =
      \begin{pmatrix}
        \Sigma_{1} & \Sigma_{12} \\
        \Sigma_{21} & \Sigma_{2}
      \end{pmatrix}
    =
      \begin{pmatrix}
        1.5 & \color{red}\rho = 1.1 \\
        \color{red}\rho = 1.1 & 1.5
      \end{pmatrix}
  \end{equation*}
% 
  \begin{figure}
    \includegraphics[scale=0.4]{gaussian_2d_rho_high.pdf}
  \end{figure}
\end{frame}

% d=2, zero correlation
\begin{frame}
  Cas $d=2$:
  %
  \begin{equation*}
    \vectorx{\mu}
    =
    \begin{pmatrix}
      \mu_1 \\
      \mu_2
    \end{pmatrix},
    \quad
    \matrixx{\Sigma}
    =
      \begin{pmatrix}
        \Sigma_{1} & \Sigma_{12} \\
        \Sigma_{21} & \Sigma_{2}
      \end{pmatrix}
    =
      \begin{pmatrix}
        1.5 & \color{red}\rho = 0 \\
        \color{red}\rho = 0 & 1.5
      \end{pmatrix}
  \end{equation*}
% 
  \begin{figure}
    \includegraphics[scale=0.4]{gaussian_2d_rho_null.pdf}
  \end{figure}
\end{frame}

% d=5, high correlation: scatter and index plots
\begin{frame}
  % \frametitle{\secname}
  Cas $d=2$:
  %
  \begin{equation*}
    \vectorx{\mu}
    =
    \begin{pmatrix}
      0 \\
      0 \\
      \dots
    \end{pmatrix},
    \quad
    \matrixx{\Sigma}
    =
    \begin{pmatrix}
      1.5 & 0.99 & \color{lightgray}0.98 & \color{lightgray}0.96 & \color{lightgray}0.94 \\
      0.99 & 1.5 & \color{lightgray}0.99 & \color{lightgray}0.98 & \color{lightgray}0.96 \\
      \color{lightgray}0.98 & \color{lightgray}0.99 & \color{lightgray}1.5 & \color{lightgray}0.99 & \color{lightgray}0.98 \\
      \color{lightgray}0.96 & \color{lightgray}0.98 & \color{lightgray}0.99 & \color{lightgray}1.5 & \color{lightgray}0.99 \\
      \color{lightgray}0.94 & \color{lightgray}0.96 & \color{lightgray}0.98 & \color{lightgray}0.99 & \color{lightgray}1.5
      \end{pmatrix}
  \end{equation*}
  %
  \begin{figure}[ht]
    \includegraphics[scale=0.3]{gaussian_2d_2outof5.pdf}
    $\Longleftrightarrow$
    \includegraphics[scale=0.3]{gaussian_2d_valuevsindex.pdf}
    \caption{Dimensions $j=1, 2$. Gauche: \textcolor{violet}{$10^4$} $+8$ échantillons. 
    Droite: $8$ \coloredemph{mêmes} échantillons.}
  \end{figure}
\end{frame}

% d=5, high correlation: scatter and index plots
\begin{frame}
  Cas $d=5$:
  %
  \begin{equation*}
    \vectorx{\mu}
    =
    \begin{pmatrix}
      0 \\
      0 \\
      \dots
    \end{pmatrix},
    \quad
    \matrixx{\Sigma}
    =
    \begin{pmatrix}
      1.5 & 0.99 & 0.98 & 0.96 & 0.94 \\
      0.99 & 1.5 & 0.99 & 0.98 & 0.96 \\
      0.98 & 0.99 & 1.5 & 0.99 & 0.98 \\
      0.96 & 0.98 & 0.99 & 1.5 & 0.99 \\
      0.94 & 0.96 & 0.98 & 0.99 & 1.5
      \end{pmatrix}
  \end{equation*}
  %
  Chaque courbe est un tirage de $\mathcal{N}(\vectorx{\mu} , \matrixx{\Sigma})$
  \begin{figure}[ht]
    \includegraphics[scale=0.3]{gaussian_5d_valuevsindex.pdf}
    \caption{$8$ échantillons, dimensions $j=1, \dots, 5$}
  \end{figure}
\end{frame}

% d=50, high correlation: scatter and index plots
\begin{frame}
  Cas $d=50$:
  %
  \begin{equation*}
    \vectorx{\mu}
    =
    \begin{pmatrix}
      0 \\
      0 \\
      \dots
    \end{pmatrix},
    \quad
    \matrixx{\Sigma}
    =
    \scalemath{0.65}{
      \begin{pmatrix}
        1.5 & 0.998 & 0.993 & 0.986 & 0.975 & 0.962 & 0.946 & 0.927 & \cdots \\
        0.998 & 1.5 & 0.998 & 0.993 & 0.986 & 0.975 & 0.962 & 0.946 & \cdots \\
        0.993 & 0.998 & 1.5 & 0.998 & 0.993 & 0.986 & 0.975 & 0.962 & \cdots \\
        0.986 & 0.993 & 0.998 & 1.5 & 0.998 & 0.993 & 0.986 & 0.975 & \cdots \\
        0.975 & 0.986 & 0.993 & 0.998 & 1.5 & 0.998 & 0.993 & 0.986 & \cdots \\
        0.962 & 0.975 & 0.986 & 0.993 & 0.998 & 1.5 & 0.998 & 0.993 & \cdots \\
        0.946 & 0.962 & 0.975 & 0.986 & 0.993 & 0.998 & 1.5 & 0.998 & \cdots \\
        0.927 & 0.946 & 0.962 & 0.975 & 0.986 & 0.993 & 0.998 & 1.5   \\
        \hdots & \hdots & \hdots & \hdots & \hdots & \hdots & \hdots & \hdots & 1.5
      \end{pmatrix}
    }
  \end{equation*}
  Chaque courbe est un tirage de $\mathcal{N}(\vectorx{\mu} , \matrixx{\Sigma})$
  %
  \begin{figure}[ht]
    \includegraphics[scale=0.3]{gaussian_50d_valuevsindex.pdf}
    \caption{$8$ échantillons, dimensions $j=1, \dots, 50$}
  \end{figure}
\end{frame}
%

% GP definition
\begin{frame}
  \frametitle{\secname}
  Cas $d=\infty$: chaque courbe est une \coloredemph{collection infinie} de valeurs, que l'on peut voir comme une fonction:
  %
  \begin{enumerate}
    \item $\vectorx{\mu}$: vecteur de taille infinie $\Leftrightarrow$ fonction moyenne $\mathbb{E}(y(x)) = m(\vectorx{x})$
    \item $\matrixx{\Sigma}$: matrice de taille infinie $\Leftrightarrow$  noyau de covariance 
    $\mathbb{C}( y(\vectorx{x}), y(\vectorx{x^\prime}) ) = k(\vectorx{x}, \vectorx{x^{\prime}})$
  \end{enumerate}
  % 
  \pause
  % 
  \begin{block}{Processus Gaussien $y(\vectorx{x}) \sim GP(m, k)$}
    Un processus Gaussien (GP) est une distribution sur des \coloredemph{fonctions} aléatoires,
   $y(\vectorx{x})$, telle que toute collection finie
    $[ y(x_i), \dots, y(x_j), \dots y(x_k) ], \forall i,j,k$ suive une distribubtion Gaussienne. 
    Les deux fonctions $m$ et $k$ définissent les paramètres du $GP$.
  \end{block}
  \pause
  Exemple, $y(t): \mathbb{R} \to \mathbb{R}$ (série temporelle), \coloredemph{n'importe qu'elle discrétisation} de $y$ doit
   former un vecteur Gaussien, en prenant par ex $2$ entrées de $y$:
  \begin{equation*}
    y(\vectorx{x}) \sim GP(m, k) \implies
    y([t_i, t_j]) \sim \mathcal{N} (
      % mu
      \begin{pmatrix}
        \mu_i \\
        \mu_j 
      \end{pmatrix},
      % sigma
      \begin{pmatrix}
        \Sigma_{i}^2 & \Sigma_{ij} \\
        \Sigma_{ij} & \Sigma_{j}^2
      \end{pmatrix}
      )
  \end{equation*}
  %
\end{frame}

% informal details
\begin{frame}
  \frametitle{\secname}
  \begin{itemize}
    \item On impose souvent une forme simple sur $\mathbb{E}(y(x)) = m(x)$ 
    comme: $\vectorx{\beta}^\top \vectorx{x}$, $\vectorx{0}$
    \pause
    \item $k(\vectorx{x}, \vectorx{x^{\prime}}) = \mathbb{C}( y(\vectorx{x}), y(\vectorx{x^\prime}) )$
     représente la \coloredemph{similarité de $y$} entre deux entrées $\vectorx{x}$ et $\vectorx{x}^\prime$,
     donc doit générer une matrice de covariance valide
     \footnote{
      $\sum_{i,j}^{n} a_i a_j k(\vectorx{x}_i, \vectorx{x}_j) \geq 0, \forall \vectorx{a} = [a_1, \dots, a_n]$
    }
    \pause
    \item $y: \mathcal{X} \to \mathcal{Y}$ est une \coloredemph{fonction}: il suffit de bien choisir $\mathcal{X}$ et $\mathcal{Y}$
    \begin{itemize}
      \item $\mathcal{X} = \mathbb{R}^{+}$ et $\mathcal{Y} = \mathbb{R}$ $\implies$
      $y$ = \coloredemph{série temporelle}
      \pause
      \item $\mathcal{X} = [a, b] \times [c, d]$ et  $\mathcal{Y} = \mathbb{R}$ $\implies$
      $y$ = \coloredemph{champ spatial} (météo, hydro, etc)
      \pause
      \item $\mathcal{X} = [a, b] \times [c, d] \times \mathbb{R}^{+}$ et $\mathcal{Y} = \mathbb{R}$ $\implies$
      $y$ = \coloredemph{série temporelle d'images}
      \pause
      \item $\mathcal{X} = \mathbb{R}^{d}$ et $\mathcal{Y} = \{1 \dots K \}$ $\implies$
      $y$ = \coloredemph{classificateur} d'images/pixels (ex: segmentation, catégorisation)
      \item \dots 
    \end{itemize}
    \pause
    \item $k$ défini \coloredemph{implicitement} les propriétés fonctionnelles de $y$
    \footnote{
      C-à-d être un noyau reproduisant d'un espace de Hilbert:
      $y(x_0) = \sum_{i=1}^{\infty} y(x_i) k(x_i, x_0)$ 
    }
    \pause
    \item Le nerf de la guerre est dans le kernel (noyau)
  \end{itemize}
  % 
\end{frame}

% Examples of kernel functions
\begin{frame}
  \frametitle{\secname}
  \begin{itemize}
    \item Quelques exemples de $GP$ avec $\mathcal{X} = \mathbb{R}$ et $\mathcal{Y} = \mathbb{R}$
    \item Série temporelle
    \item Bonne manière de s'approprier les noyaux (et dépasser la théorie...)
  \end{itemize}
  
\end{frame}


% Exponential quadratic
\begin{frame}
  \frametitle{\secname}
  
  % 
  \begin{itemize}
    \item kernel exponentiel quadratique (aka "Radial basis", "Gaussian", "Heat" kernel):
    $k (t, t^\prime) = e^{- \frac{(t - t^\prime)^2}{2 \ell^2} }$
  \end{itemize}
  %
  \begin{figure}
    \includegraphics{10_gp_time_SquaredExponentialKernel_2.00.pdf}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{\secname}

  % 
  \begin{itemize}
    \item Exponentiel quadratique (aka "Radial basis", "Gaussian", "Heat" kernel):
    $k (t, t^\prime) = e^{- \frac{(t - t^\prime)^2}{2 \ell^2} }$
  \end{itemize}
  %
  \begin{figure}
    \includegraphics{10_gp_time_SquaredExponentialKernel_1.00.pdf}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{\secname}
  
  % 
  \begin{itemize}
    \item Exponentiel quadratique (aka "Radial basis", "Gaussian", "Heat" kernel):
    $k (t, t^\prime) = e^{- \frac{(t - t^\prime)^2}{2 \ell^2} }$
    \item $GP (0, k)$ génères des $y$ infiniment différentiables par rapport à $t$
    \item \implies $\ell$ contrôle le lissé (ici au sens oscillatoir) des générées
  \end{itemize}
  %
  \begin{figure}
    \includegraphics{10_gp_time_SquaredExponentialKernel_0.10.pdf}
  \end{figure}
\end{frame}

% Matern kernel
\begin{frame}
  \frametitle{\secname}
  
  % 
  \begin{itemize}
    \item kernel de Matérn:
    $k (t, t^\prime) = ( 1 + \sqrt{\frac{3 |t - t^\prime|}{\ell} } ) e^{-\sqrt{\frac{3 |t - t^\prime|}{\ell} }}$
  \end{itemize}
  %
  \begin{figure}
    \includegraphics{10_gp_time_MaternKernel_2.00.pdf}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{\secname}
  
  % 
  \begin{itemize}
    \item kernel de Matérn:
    $k (t, t^\prime) = ( 1 + \sqrt{\frac{3 |t - t^\prime|}{\ell} } ) e^{-\sqrt{\frac{3 |t - t^\prime|}{\ell} }}$
  \end{itemize}
  %
  \begin{figure}
    \includegraphics{10_gp_time_MaternKernel_1.00.pdf}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{\secname}
  
  % 
  \begin{itemize}
    \item kernel de Matérn:
    $k (t, t^\prime) = ( 1 + \sqrt{\frac{3 |t - t^\prime|}{\ell} } ) e^{-\sqrt{\frac{3 |t - t^\prime|}{\ell} }}$
    \item Génère des $y$ différentiables une seule fois par rapport à $t$
    \item $\ell$: même interprétation que le kernel exponentiel quadratique
  \end{itemize}
  %
  \begin{figure}
    \includegraphics{10_gp_time_MaternKernel_0.10.pdf}
  \end{figure}
\end{frame}

% Locally periodic
\begin{frame}
  \frametitle{\secname}
  
  % 
  \begin{itemize}
    \item kernel locallement-périodique et exponentiel:
    $k (t, t^\prime) = e^{- \frac{2 \sin^2(\pi (t - t^\prime) / p)}{\ell^2}} e^{- \frac{(t - t^\prime)^2}{2 \ell^2}} $
  \end{itemize}
  %
  \begin{figure}
    \includegraphics{10_gp_time_LocallyPeriodicKernel_1.25.pdf}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{\secname}
  
  % 
  \begin{itemize}
    \item kernel locallement-périodique et exponentiel:
    $k (t, t^\prime) = e^{- \frac{2 \sin^2(\pi (t - t^\prime) / p)}{\ell^2}} e^{- \frac{(t - t^\prime)^2}{2 \ell^2}} $
    \item Génère des $y$ infiniment différentiables et quasi-périodiques
  \end{itemize}
  %
  \begin{figure}
    \includegraphics{10_gp_time_LocallyPeriodicKernel_0.50.pdf}
  \end{figure}
\end{frame}

%========== Motivations ==================%
% \section{Motivations}
% Où est-ce rencontre-t-on des processus stochastiques ?
% Gaussiens ?

\section[short]{}
\end{document}